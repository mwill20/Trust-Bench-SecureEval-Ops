# Project 2 ‚Äì Multi-Agent Repository Auditor

This directory contains the minimal, self-contained implementation that fulfils the Project&nbsp;2 rubric. Everything you need to run and evaluate the multi-agent workflow lives here.

## ü§ù Enhanced Agent Collaboration

Our multi-agent system features **intelligent collaboration** where agents actively communicate and influence each other's assessments:

### Cross-Agent Intelligence
- **SecurityAgent ‚Üí QualityAgent**: Quality scores are automatically reduced when security issues are detected (up to 25-point penalty)
- **QualityAgent ‚Üí DocumentationAgent**: Documentation assessment incorporates code quality metrics (file counts, test coverage)
- **SecurityAgent ‚Üí DocumentationAgent**: Documentation scores are penalized when security issues aren't addressed in docs
- **Direct Agent Communication**: Agents send contextual messages to each other with structured data

### Collaborative Decision Making
- Quality Agent adjusts scoring based on security findings: `"Adjusted quality score down by 25 points due to 5 security finding(s) from SecurityAgent"`
- Documentation Agent considers both quality metrics and security context: `"No tests found by QualityAgent - documentation lacks testing info"`
- All cross-agent communications are logged in detailed conversation histories
- Final scores reflect collaborative intelligence, not just individual tool outputs

### Example Collaboration Flow
```
SecurityAgent ‚Üí QualityAgent: "FYI: Found 5 security issues that may impact quality assessment"
QualityAgent ‚Üí SecurityAgent: "Incorporated your 5 security findings into quality assessment"
DocumentationAgent ‚Üí SecurityAgent: "Noted your 5 findings - documentation lacks security guidance"
```

## üöÄ Quick Start - Choose Your Interface

### Option 1: Interactive Launcher (Recommended)
Double-click `launch.bat` for a menu with multiple options:
- Command Line Interface
- Web Interface 
- Quick analysis presets

### Option 2: Web Interface
```powershell
python web_interface.py
```
Then open http://localhost:5000 in your browser for a beautiful web UI with real-time collaboration visualization and an LLM-powered chat panel.

### Option 3: Command Line
```powershell
python main.py --repo .. --output output
```

### Option 4: Batch Scripts
```powershell
# Windows Batch
run_audit.bat .                    # Analyze current directory
run_audit.bat ..                   # Analyze parent directory  
run_audit.bat "C:\path\to\repo"    # Analyze specific path

# PowerShell
.\run_audit.ps1 .                  # Analyze current directory
.\run_audit.ps1 .. my_output       # Custom output directory
```

## What's Inside

- `main.py` ‚Äì CLI entry point that wires LangGraph, executes the workflow, and writes reports.
- `web_interface.py` ‚Äì Beautiful web UI with real-time agent progress and collaboration visualization
- `launch.bat` ‚Äì Interactive launcher with multiple interface options
- `run_audit.bat` / `run_audit.ps1` ‚Äì Convenient wrapper scripts
- `multi_agent_system/` ‚Äì Lightweight package with collaborative agents, orchestration logic, and reporting helpers.
  - `agents.py` ‚Äì SecurityAgent, QualityAgent, DocumentationAgent with cross-communication
  - `orchestrator.py` ‚Äì LangGraph workflow with collaboration tracking
  - `tools.py` ‚Äì Secret scanning, structure analysis, documentation review tools
  - `types.py` ‚Äì Shared data structures and messaging protocols
- `requirements.txt` ‚Äì Focused dependency list (LangGraph, Flask, Requests).

## Setup

```powershell
python -m venv .venv          # optional but recommended
.\.venv\Scripts\activate
pip install -r requirements.txt
```

If you already installed dependencies at the repository root, you can reuse that environment.

### Environment Variables for LLM Chat

Configure at least one provider before launching the web interface so the chat panel can answer questions. The helper inspects these environment variables:

- `LLM_PROVIDER` (optional) ‚Äì defaults to `openai`; choose `openai`, `groq`, or `gemini`
- `OPENAI_API_KEY` (optional) ‚Äì required when `LLM_PROVIDER` is `openai`; override model with `OPENAI_MODEL`
- `GROQ_API_KEY` (optional) ‚Äì required when `LLM_PROVIDER` is `groq`; override model with `GROQ_MODEL` (defaults to `llama-3.1-8b-instant`)
- `GEMINI_API_KEY` (optional) ‚Äì required when `LLM_PROVIDER` is `gemini`; override model with `GEMINI_MODEL`

Example (PowerShell):

```powershell
$env:OPENAI_API_KEY = "sk-..."
$env:LLM_PROVIDER = "openai"
```

Only one provider key is required at a time; add more keys if you plan to switch providers from the dropdown.

### Security Features (optional)

Enable incremental validation and sanitization by setting the `ENABLE_SECURITY_FILTERS` environment variable (defaults to `true`). This routes all repository URLs through stricter checks and sanitises chat prompts before they reach the backend. Toggle it off (`0`, `false`, `off`) if you need to diagnose issues or compare behaviour.

If you prefer not to set environment variables, you can paste a key into the web UI chat panel. The key stays in your browser session and is only transmitted when you test the connection or send a chat request.

### LLM Utility Module

The `llm_utils.py` helper centralizes chat completions for Groq, OpenAI, and Gemini. It relies on the environment variables above and keeps API calls out of the Flask views so providers can be swapped without code changes.

## Agent Collaboration in Action

### üõ°Ô∏è SecurityAgent (Proactive Communication)
- Scans for AWS keys, GitHub tokens, private keys, etc.
- **Collaborates by**: Immediately notifying other agents of security findings
- **Impact**: Influences quality and documentation scoring based on security context

### üîç QualityAgent (Security-Aware Assessment)  
- Analyzes code structure, languages, and test coverage
- **Collaborates by**: Adjusting scores based on SecurityAgent findings
- **Impact**: Reduces quality score when security issues detected (more realistic assessment)

### üìù DocumentationAgent (Context-Aware Scoring)
- Reviews README files, counts sections, checks completeness
- **Collaborates by**: Using QualityAgent metrics and SecurityAgent alerts
- **Impact**: Penalizes documentation that doesn't address testing or security gaps

### ü§ñ Manager (Collaboration Orchestrator)
- Coordinates all agents and tracks cross-communications
- **Reports**: Number of collaborative interactions and their impact on final scores
- **Transparency**: Full conversation log showing agent-to-agent decision making

## Example Collaborative Results

### Collaboration Metrics from Recent Run:
```
Grade: NEEDS_ATTENTION (30.0 pts)
Agents collaborated on 5 cross-communications:
- Security findings influenced quality and documentation scores
- Quality assessment incorporated security analysis  
- Documentation score adjusted based on quality metrics
```

### Individual Agent Adjustments:
- **QualityAgent**: `"Adjusted for 5 security findings"` (25-point reduction)
- **DocumentationAgent**: `"Adjusted based on quality metrics"` + `"Adjusted for security gaps"` (10-point total reduction)

## Example Results

### Trust_Bench_Clean Analysis (Collaborative Assessment)
- **Overall Score:** ~30/100 ("needs_attention")
- **Security:** 5 potential secrets detected ‚Üí Influences other agents
- **Quality:** 53 files, 0% test coverage ‚Üí Penalizes documentation
- **Documentation:** Good content but lacks security/testing guidance ‚Üí Score reduced

### Project2v2 Self-Analysis (Collaborative Assessment)
- **Overall Score:** ~61/100 ("fair") 
- **Security:** Clean (minimal findings) ‚Üí No cross-agent penalties
- **Quality:** 53 files, no dedicated tests ‚Üí Affects documentation assessment
- **Documentation:** Good README but collaboration reveals testing gaps

## Deliverables Checklist (Rubric Mapping)

- ‚úÖ **Main orchestrator script**: `main.py`
- ‚úÖ **At least 3 agents**: Manager, SecurityAgent, QualityAgent, DocumentationAgent
- ‚úÖ **At least 3 tools**: secret scan, repository structure analysis, documentation review
- ‚úÖ **Inter-agent communication**: Enhanced with direct agent-to-agent messaging and collaborative scoring
- ‚úÖ **Tangible output**: JSON + Markdown summary in the output directory with collaboration metrics
- ‚úÖ **README & requirements**: this document with clear setup/run steps
- ‚≠ê **Security emphasis**: high-signal credential scanner with cross-agent impact
- üéØ **User-friendly interfaces**: Web UI, batch scripts, interactive launcher
- ü§ù **Advanced Collaboration**: Agents influence each other's assessments for more intelligent results

## Command Line Usage

- `--repo` (optional) points to the repository you want to audit. Defaults to the parent directory (`..`).  
- `--output` (optional) sets where the generated `report.json` and `report.md` should be written. Defaults to `./output`.

Example console output showing collaboration:

```
=== Multi-Agent Evaluation Complete ===
Repository: C:\Projects\Trust_Bench_Clean\Project2v2
Overall Score: 30.0
Grade: needs_attention
Report (JSON): C:\Projects\Trust_Bench_Clean\Project2v2\collaboration_test\report.json
Report (Markdown): C:\Projects\Trust_Bench_Clean\Project2v2\collaboration_test\report.md
```

## What Makes This Special

üß† **Intelligent Collaboration**: Unlike simple multi-agent systems, our agents actually influence each other's decisions in real-time

üìä **Contextual Scoring**: Final scores reflect cross-domain insights (security impacts quality, quality impacts documentation)

üîç **Full Transparency**: Complete conversation logs show exactly how agents collaborated to reach their assessments

üöÄ **Multiple Interfaces**: From simple double-click launcher to sophisticated web UI with real-time progress

You now have a clean, runnable package that demonstrates advanced multi-agent collaboration and can be copied, zipped, or submitted independently of the legacy Trust Bench stack.
