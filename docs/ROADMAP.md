# ğŸ§­ TrustBench Development Roadmap & Rubric

This roadmap keeps the project aligned as it evolves from a working baseline into a full, deployable product.

---

## ğŸ§© PHASE 0 â€” Baseline Validation (âœ… Completed)

**Goal:** Ensure core engine, reports, tests, and CI all function deterministically.

### Deliverables
- [x] Core agents wired and testable (`task_fidelity`, `security_eval`, `system_perf`, `ethics_refusal`)
- [x] Orchestrator produces metrics, gate.json, and report.html
- [x] CI workflows split: lint/tests + eval gate
- [x] Datasets, references, and examples consolidated with READMEs
- [x] Pytest passes locally and in CI

### Rubric Criteria
| Category | Metric | Threshold |
|-----------|---------|-----------|
| Test Coverage | â‰¥ 90% core files | âœ… |
| CI Stability | 100% runs succeed | âœ… |
| Documentation Clarity | Root + core README coherent, current | âœ… |
| Artifacts | report.html + metrics.json reproducible | âœ… |

---

## âš™ï¸ PHASE 1 â€” Real Agent Logic (In Progress)

**Goal:** Replace deterministic stubs with real model/tool integrations.

### Deliverables
- [ ] `task_fidelity` â†’ RAGAS / retrieval accuracy check
- [ ] `security_eval` â†’ PromptGuard, Semgrep, secrets scan (MCP-backed)
- [ ] `system_perf` â†’ true latency tracking
- [ ] `ethics_refusal` â†’ Groq LLM judge with refusal rubric
- [ ] Adjust thresholds to meaningful production levels
- [ ] Expand `profiles/highstakes.yaml` with realistic CI gate limits

### Rubric Criteria
| Category | Measure | Target |
|-----------|----------|--------|
| Integration Success | All agents execute without stubs | âœ… |
| Metric Validity | Scores vary realistically (non-static) | âœ… |
| Threshold Accuracy | <5% false failures over 10 runs | âœ… |
| Security Coverage | Injection-block, secrets-scan, refusal tests measurable | âœ… |

---

## ğŸ§± PHASE 2 â€” Studio App (TrustBench Studio)

**Goal:** Create a usable web interface for running and visualizing evaluations.

### Deliverables
- [ ] Streamlit app reads live reports (`trustbench_core/eval/runs/latest/`)
- [ ] User inputs repo URL or uploads dataset
- [ ] Scorecards with color-coded pillars
- [ ] Failure table with top artifacts and â€œSuggested Fixâ€ hints
- [ ] Profile selector (default/highstakes/custom)
- [ ] Export: HTML/CSV/Markdown reports
- [ ] Optional: Login/session state for saved runs

### Rubric Criteria
| UX Category | Metric | Target |
|--------------|---------|--------|
| Usability | User runs eval in <60 sec from open | âœ… |
| Visibility | Metrics auto-refresh after run | âœ… |
| Clarity | Pass/fail visually distinct, labeled | âœ… |
| Reliability | No Streamlit crashes or subprocess errors | âœ… |

---

## ğŸ§® PHASE 3 â€” CI/CD + Gating Automation

**Goal:** Turn TrustBench into a CI-enforceable safety gate and analytics stream.

### Deliverables
- [ ] Enable `eval.yml` on default branch with required checks
- [ ] Auto-upload HTML/CSV artifacts for dashboards
- [ ] Slack or email notifications on CI failure
- [ ] Scheduled nightly â€œtrust regressionâ€ runs
- [ ] Archive runs to `trustbench_core/eval/runs/history/`

### Rubric Criteria
| Area | Measure | Target |
|-------|----------|--------|
| Automation | CI auto-triggers eval on PR | âœ… |
| Artifact Retention | Reports for last 30 runs preserved | âœ… |
| Feedback Speed | <5 minutes to report completion | âœ… |
| Branch Protection | Gate enforced in GitHub settings | âœ… |

---

## ğŸ” PHASE 4 â€” Evaluation Quality & Human Loop

**Goal:** Calibrate AI judgments with human feedback (HITL).

### Deliverables
- [ ] `hitl/labels.jsonl` with 30+ annotated examples
- [ ] `trustbench_core/eval/evaluate_agent.py` supports `--apply-hitl`
- [ ] `docs/playbooks/hitl.md` defines labeling rubric (faithfulness, refusal, safety)
- [ ] Comparison dashboard showing model vs. human agreement

### Rubric Criteria
| Aspect | Metric | Target |
|---------|---------|--------|
| Human Agreement | â‰¥ 85% alignment between judge & human | âœ… |
| Label Coverage | â‰¥ 30 samples per pillar | âœ… |
| Bias Check | <10% systematic deviation | âœ… |
| Documentation | HITL rubric published and versioned | âœ… |

---

## ğŸ“Š PHASE 5 â€” Trust Metrics Dashboard

**Goal:** Turn TrustBench data into a visual trend dashboard for long-term evaluation.

### Deliverables
- [ ] Aggregate metrics over time (faithfulness, refusal accuracy, latency)
- [ ] Trend chart (Streamlit or lightweight React)
- [ ] Comparative profiles (default vs highstakes vs experimental)
- [ ] Exportable CSV/JSON summaries for external dashboards

### Rubric Criteria
| Category | Measure | Target |
|-----------|----------|--------|
| Trend Accuracy | Data matches report.json history | âœ… |
| Visual Clarity | Charts update per new CI run | âœ… |
| Extensibility | New metric types pluggable without refactor | âœ… |

---

## ğŸ PHASE 6 â€” Publication-Ready Release

**Goal:** Deliver a stable, documented, demoable system.

### Deliverables
- [ ] Clean project tree with minimal redundancy
- [ ] Example notebook + video walkthrough
- [ ] LICENSE + CITATION.cff
- [ ] `CHANGELOG.md` for releases
- [ ] Deployed Studio app (Render, HuggingFace, or Streamlit Cloud)
- [ ] Final â€œTrustBench Overviewâ€ publication (markdown/pdf)

### Rubric Criteria
| Category | Measure | Target |
|-----------|----------|--------|
| Reproducibility | Full repo runs with one command | âœ… |
| Documentation | All READMEs current and linked | âœ… |
| Deployability | Streamlit app launches cleanly | âœ… |
| Presentation | Publication-quality artifact | âœ… |

---

## ğŸ“‹ Rolling Checklist Summary

```
[âœ“] Baseline validation complete
[ ] Real agent logic integrated
[ ] Studio app live and interactive
[ ] CI gating enforced
[ ] HITL workflow active
[ ] Metrics dashboard implemented
[ ] Final release deployed
```

---

### Maintainer Notes
- Update this roadmap each milestone or release tag.
- Each phase completion should close with a CI-verified run + updated README badge.
- All future features should trace to one phase goal in this roadmap.
